#!/usr/bin/env python3
"""
Production-Ready Ultra-Enhanced Model
Fixed prediction interface for deployment
"""

import pandas as pd
import numpy as np
import re
import warnings
from sklearn.model_selection import train_test_split  # LINE 10: removed cross_val_score import
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC                          # NEW: added SVM for better accuracy
from sklearn.calibration import CalibratedClassifierCV    # NEW: needed for SVM soft voting
from sklearn.metrics import classification_report, accuracy_score
import pickle
import json
from datetime import datetime
from scipy.sparse import hstack

warnings.filterwarnings('ignore')

class ProductionExpenseClassifier:
    """Production-ready classifier with simplified interface"""
    def __init__(self, model, tfidf, scaler, numeric_features, label_encoder):
        self.model = model
        self.tfidf = tfidf
        self.scaler = scaler
        self.numeric_features = numeric_features
        self.label_encoder = label_encoder
        
        self.keyword_categories = {
            'food': ['food', 'restaurant', 'cafe', 'meal', 'lunch', 'dinner', 'breakfast', 
                    'snacks', 'grocery', 'milk', 'bread', 'delivery', 'dining', 'pizza', 
                    'burger', 'chicken', 'rice', 'curry', 'naan', 'biryani'],
            'transport': ['auto', 'taxi', 'train', 'bus', 'metro', 'fuel', 'gas', 'parking', 
                         'uber', 'ola', 'transport', 'vehicle', 'car'],
            'bills': ['bill', 'electric', 'electricity', 'water', 'internet', 'phone', 
                     'mobile', 'subscription', 'utility', 'service'],
            'shopping': ['shopping', 'store', 'mall', 'amazon', 'flipkart', 'clothes', 'electronics'],
            'health': ['hospital', 'doctor', 'pharmacy', 'medical', 'health', 'clinic'],
            'entertainment': ['movie', 'cinema', 'game', 'netflix', 'entertainment'],
            'tools': ['tool', 'tools', 'equipment', 'hardware', 'saw', 'hammer', 'drill', 'wrench', 
                     'stanley', 'bosch', 'makita', 'dewalt', 'craftsman', 'precision', 'manufacturing', 
                     'workshop', 'machinery', 'component', 'spare', 'automatic', 'manual', 'industrial'],
            'business': ['business', 'office', 'consulting', 'professional', 'legal', 'accounting', 
                        'service', 'company', 'corporate', 'commercial']
        }
        
    def _clean_text(self, text):
        text = str(text).lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = ' '.join(text.split())
        return text
    
    def _count_keywords(self, text, keywords):
        text_lower = text.lower()
        return sum(1 for keyword in keywords if keyword in text_lower)
    
    def _extract_features(self, note, amount):
        note_clean = self._clean_text(note)
        
        features = {
            'Amount': amount,
            'LogAmount': np.log1p(amount),
            'AmountRange': self._get_amount_range(amount),
            'DayOfWeek': pd.Timestamp.now().dayofweek,
            'Month': pd.Timestamp.now().month,
            'Day': pd.Timestamp.now().day,
            'IsWeekend': 1 if pd.Timestamp.now().dayofweek >= 5 else 0,
            'IsMonthEnd': 1 if pd.Timestamp.now().day >= 25 else 0,
            'IsMonthStart': 1 if pd.Timestamp.now().day <= 5 else 0,
            'TextLength': len(note_clean),
            'WordCount': len(note_clean.split()),
            'UpperCaseRatio': sum(1 for c in note if c.isupper()) / len(note) if note else 0,
            'DigitRatio': sum(1 for c in note if c.isdigit()) / len(note) if note else 0,
        }
        
        for category, keywords in self.keyword_categories.items():
            features[f'{category}_keywords'] = self._count_keywords(note_clean, keywords)
        
        features['HasAmountPattern'] = 1 if re.search(r'\d+\s*(rs|rupees|inr|\$)', note.lower()) else 0
        features['HasTimePattern'] = 1 if re.search(r'\d{1,2}:\d{2}', note) else 0
        features['HasPlacePattern'] = 1 if re.search(r'place\s+\d+', note.lower()) else 0
        
        return note_clean, features
    
    def _get_amount_range(self, amount):
        if amount < 50:
            return 0
        elif amount < 200:
            return 1
        elif amount < 500:
            return 2
        elif amount < 1000:
            return 3
        elif amount < 5000:
            return 4
        else:
            return 5
    
    def predict(self, data):
        if isinstance(data, pd.DataFrame):
            note = data['Note'].iloc[0] if 'Note' in data.columns else ''
            amount = data['Amount'].iloc[0] if 'Amount' in data.columns else 0
        else:
            note = data.get('Note', '') if isinstance(data, dict) else ''
            amount = data.get('Amount', 0) if isinstance(data, dict) else 0

        note_clean, numeric_features_dict = self._extract_features(note, amount)
        text_features = self.tfidf.transform([note_clean])

        numeric_array = []
        for feat in self.numeric_features:
            if feat in numeric_features_dict:
                numeric_array.append(numeric_features_dict[feat])
            else:
                numeric_array.append(0)

        if len(numeric_array) != len(self.numeric_features):
            if len(numeric_array) < len(self.numeric_features):
                numeric_array.extend([0] * (len(self.numeric_features) - len(numeric_array)))
            else:
                numeric_array = numeric_array[:len(self.numeric_features)]

        numeric_scaled = self.scaler.transform([numeric_array])
        X_combined = hstack([text_features, numeric_scaled])

        prediction_encoded = self.model.predict(X_combined)[0]
        prediction = self.label_encoder.inverse_transform([prediction_encoded])[0]
        
        return prediction


def train_production_model():
    print("ðŸš€ Training Production-Ready Ultra Model")
    print("=" * 60)
    
    # â”€â”€ LOAD DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df = pd.read_csv("../data/exp.csv")
    print(f"Dataset: {df.shape}")
    
    # â”€â”€ CLEAN DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_clean = df.copy()
    df_clean = df_clean.dropna(subset=['Category'])
    df_clean = df_clean[df_clean['Category'].str.strip() != '']
    df_clean['Amount'] = pd.to_numeric(df_clean['Amount'], errors='coerce')
    df_clean = df_clean[df_clean['Amount'] > 0]
    df_clean['Note'] = df_clean['Note'].fillna('Unknown Transaction').astype(str)
    
    # â”€â”€ CATEGORY MAPPING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    category_mapping = {
        'Food': 'Food & Dining', 'food': 'Food & Dining', 'Dinner': 'Food & Dining',
        'Lunch': 'Food & Dining', 'breakfast': 'Food & Dining', 'Grocery': 'Food & Dining',
        'snacks': 'Food & Dining', 'Milk': 'Food & Dining', 'Ice cream': 'Food & Dining',
        'Transportation': 'Transportation', 'Train': 'Transportation', 'auto': 'Transportation',
        'subscription': 'Bills & Utilities', 'Household': 'Bills & Utilities',
        'Family': 'Personal & Family', 'Festivals': 'Personal & Family',
        'Salary': 'Income', 'Interest': 'Income', 'Dividend earned on Shares': 'Income',
        'Other': 'Miscellaneous', 'Apparel': 'Apparel', 'Gift': 'Gift',
        'Healthcare': 'Healthcare', 'Medical/Healthcare': 'Healthcare'
    }
    df_clean['Category'] = df_clean['Category'].replace(category_mapping)
    
    # Keep categories with enough samples
    category_counts = df_clean['Category'].value_counts()
    valid_categories = category_counts[category_counts >= 20].index
    df_clean = df_clean[df_clean['Category'].isin(valid_categories)]
    
    print(f"Cleaned: {df_clean.shape}")
    print(f"Categories: {sorted(df_clean['Category'].unique())}")
    
    # â”€â”€ FEATURE ENGINEERING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def clean_text(text):
        text = str(text).lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        return ' '.join(text.split())
    
    def count_keywords(text, keywords):
        return sum(1 for kw in keywords if kw in text.lower())
    
    df_clean['Note_clean'] = df_clean['Note'].apply(clean_text)
    
    # Amount features
    df_clean['LogAmount'] = np.log1p(df_clean['Amount'])
    df_clean['AmountRange'] = pd.cut(df_clean['Amount'], 
                                     bins=[0, 50, 200, 500, 1000, 5000, float('inf')],
                                     labels=[0, 1, 2, 3, 4, 5]).astype(int)
    df_clean['AmountSquared'] = df_clean['Amount'] ** 2          # NEW: extra amount feature
    df_clean['LogAmountSquared'] = df_clean['LogAmount'] ** 2    # NEW: extra amount feature
    
    # Text features
    df_clean['TextLength'] = df_clean['Note_clean'].str.len()
    df_clean['WordCount'] = df_clean['Note_clean'].str.split().str.len()
    df_clean['UpperCaseRatio'] = df_clean['Note'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if x else 0)
    df_clean['DigitRatio'] = df_clean['Note'].apply(lambda x: sum(1 for c in x if c.isdigit()) / len(x) if x else 0)
    df_clean['UniqueWordRatio'] = df_clean['Note_clean'].apply(   # NEW: unique word ratio
        lambda x: len(set(x.split())) / len(x.split()) if x.split() else 0
    )
    
    # Keyword features â€” EXPANDED keyword lists for better accuracy
    keyword_categories = {
        'food': ['food', 'restaurant', 'chicken', 'pizza', 'meal', 'dining', 'naan', 'curry',
                 'biryani', 'lunch', 'dinner', 'breakfast', 'cafe', 'snack', 'burger', 'rice',
                 'bread', 'milk', 'grocery', 'swiggy', 'zomato', 'hotel', 'dhaba', 'mess'],
        'transport': ['taxi', 'auto', 'fuel', 'parking', 'uber', 'transport', 'gas', 'metro',
                      'bus', 'train', 'ola', 'rapido', 'petrol', 'diesel', 'flight', 'ticket',
                      'cab', 'vehicle', 'toll', 'railway', 'redbus', 'makemytrip'],
        'bills': ['bill', 'electric', 'internet', 'phone', 'subscription', 'utility',
                  'recharge', 'broadband', 'wifi', 'airtel', 'jio', 'bsnl', 'vodafone',
                  'electricity', 'water', 'gas bill', 'postpaid', 'prepaid', 'dth'],
        'shopping': ['shopping', 'amazon', 'store', 'clothes', 'electronics', 'mall',
                     'flipkart', 'meesho', 'myntra', 'ajio', 'nykaa', 'purchase', 'buy',
                     'market', 'retail', 'shop', 'order', 'delivery'],
        'health': ['hospital', 'doctor', 'medical', 'health', 'pharmacy', 'clinic',
                   'medicine', 'apollo', 'fortis', 'diagnostic', 'lab', 'test', 'scan',
                   'dental', 'eye', 'surgery', 'consultation', 'prescription'],
        'entertainment': ['movie', 'game', 'entertainment', 'netflix', 'cinema',
                          'amazon prime', 'hotstar', 'spotify', 'concert', 'event',
                          'pvr', 'inox', 'bookmyshow', 'show', 'ticket'],
        'tools': ['tool', 'tools', 'equipment', 'hardware', 'saw', 'hammer', 'drill',
                  'wrench', 'stanley', 'bosch', 'makita', 'precision', 'manufacturing',
                  'workshop', 'machinery', 'industrial', 'spare', 'component'],
        'business': ['business', 'office', 'consulting', 'professional', 'service',
                     'company', 'corporate', 'legal', 'accounting', 'invoice', 'vendor']
    }
    
    for category, keywords in keyword_categories.items():
        df_clean[f'{category}_keywords'] = df_clean['Note_clean'].apply(
            lambda x: count_keywords(x, keywords)
        )
    
    # Pattern features
    df_clean['HasAmountPattern'] = df_clean['Note'].apply(lambda x: 1 if re.search(r'\d+\s*(rs|rupees|inr)', x.lower()) else 0)
    df_clean['HasTimePattern'] = df_clean['Note'].apply(lambda x: 1 if re.search(r'\d{1,2}:\d{2}', x) else 0)
    df_clean['HasPlacePattern'] = df_clean['Note'].apply(lambda x: 1 if re.search(r'place\s+\d+', x.lower()) else 0)
    df_clean['HasUPIPattern'] = df_clean['Note'].apply(    # NEW: UPI transactions
        lambda x: 1 if re.search(r'upi|gpay|phonepe|paytm', x.lower()) else 0
    )
    
    # Temporal features
    if 'Date' in df_clean.columns:
        df_clean['Date'] = pd.to_datetime(df_clean['Date'], errors='coerce')
        df_clean = df_clean.dropna(subset=['Date'])
        df_clean['DayOfWeek'] = df_clean['Date'].dt.dayofweek
        df_clean['Month'] = df_clean['Date'].dt.month
        df_clean['Day'] = df_clean['Date'].dt.day
        df_clean['IsWeekend'] = (df_clean['DayOfWeek'] >= 5).astype(int)
        df_clean['IsMonthEnd'] = (df_clean['Day'] >= 25).astype(int)
        df_clean['IsMonthStart'] = (df_clean['Day'] <= 5).astype(int)
        df_clean['Quarter'] = df_clean['Date'].dt.quarter   # NEW: quarter feature
    
    # â”€â”€ PREPARE FEATURES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ðŸ”§ Preparing features...")
    
    # TF-IDF â€” IMPROVED settings for better accuracy
    tfidf = TfidfVectorizer(
        max_features=1000,          # was 500 â€” more features = better accuracy
        ngram_range=(1, 3),         # was (1,2) â€” added trigrams
        min_df=1,                   # was 2 â€” capture rare but important words
        max_df=0.9,
        sublinear_tf=True,          # NEW: log scaling for TF
        analyzer='word'
    )
    text_features = tfidf.fit_transform(df_clean['Note_clean'])
    
    # Numeric features â€” EXPANDED
    numeric_features = [
        'Amount', 'LogAmount', 'AmountRange', 'AmountSquared', 'LogAmountSquared',
        'TextLength', 'WordCount', 'UpperCaseRatio', 'DigitRatio', 'UniqueWordRatio',
        'food_keywords', 'transport_keywords', 'bills_keywords', 'shopping_keywords',
        'health_keywords', 'entertainment_keywords', 'tools_keywords', 'business_keywords',
        'HasAmountPattern', 'HasTimePattern', 'HasPlacePattern', 'HasUPIPattern'
    ]
    
    if 'DayOfWeek' in df_clean.columns:
        numeric_features.extend(['DayOfWeek', 'Month', 'Day', 'IsWeekend', 'IsMonthEnd', 'IsMonthStart', 'Quarter'])
    
    available_features = [feat for feat in numeric_features if feat in df_clean.columns]
    X_numeric = df_clean[available_features].fillna(0)
    
    scaler = StandardScaler()
    X_numeric_scaled = scaler.fit_transform(X_numeric)
    
    X_combined = hstack([text_features, X_numeric_scaled])
    y = df_clean['Category']
    
    print(f"Features: {X_combined.shape}")
    print(f"Categories: {y.value_counts().to_dict()}")
    
    # Label encoding
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X_combined, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )
    
    print(f"Training: {X_train.shape}, Testing: {X_test.shape}")
    
    # â”€â”€ TRAIN MODELS â€” IMPROVED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ðŸ¤– Training optimized ensemble...")
    
    models = {
        # IMPROVED: higher C and more iterations
        'logistic_regression': LogisticRegression(
            C=5.0, max_iter=3000, class_weight='balanced', solver='lbfgs', multi_class='auto'
        ),
        # IMPROVED: more trees and depth
        'random_forest': RandomForestClassifier(
            n_estimators=300, max_depth=20, min_samples_split=3,
            min_samples_leaf=1, class_weight='balanced', random_state=42, n_jobs=-1
        ),
        # IMPROVED: more estimators and lower learning rate
        'gradient_boosting': GradientBoostingClassifier(
            n_estimators=200, learning_rate=0.08, max_depth=7,
            min_samples_split=3, subsample=0.8, random_state=42
        ),
        # NEW: SVM is excellent for text classification
        'svm': CalibratedClassifierCV(
            LinearSVC(C=2.0, max_iter=3000, class_weight='balanced')
        )
    }
    
    trained_models = {}
    for name, model in models.items():
        print(f"Training {name}...")
        model.fit(X_train, y_train)
        score = model.score(X_test, y_test)
        trained_models[name] = model
        print(f"  {name}: {score:.4f} ({score*100:.2f}%)")
    
    # Create ensemble with all 4 models
    ensemble = VotingClassifier(
        estimators=[(name, model) for name, model in trained_models.items()],
        voting='soft'
    )
    
    ensemble.fit(X_train, y_train)
    ensemble_score = ensemble.score(X_test, y_test)
    
    print(f"\nðŸ† ENSEMBLE ACCURACY: {ensemble_score:.4f} ({ensemble_score*100:.2f}%)")
    
    # â”€â”€ CLASSIFICATION REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LINE 323-325: cross_val_score REMOVED (was causing the crash)
    # cv_scores = cross_val_score(ensemble, X_combined, y_encoded, cv=5)  â† REMOVED
    # print(f"CV Mean: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")   â† REMOVED
    
    y_pred = ensemble.predict(X_test)
    print("\nðŸ“‹ Classification Report:")
    unique_test_classes = np.unique(y_test)
    target_names_subset = [label_encoder.classes_[i] for i in unique_test_classes]
    print(classification_report(y_test, y_pred, labels=unique_test_classes, target_names=target_names_subset))
    
    # â”€â”€ CREATE PRODUCTION MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    production_model = ProductionExpenseClassifier(
        model=ensemble,
        tfidf=tfidf,
        scaler=scaler,
        numeric_features=available_features,
        label_encoder=label_encoder
    )
    
    # Quick test
    print("\nðŸ§ª Testing production model...")
    test_cases = [
        "CHICKEN ANGARA GARLIC NAAN restaurant food dining",
        "electricity bill payment utility service",
        "uber taxi ride transport vehicle",
        "amazon shopping electronics purchase"
    ]
    for case in test_cases:
        pred = production_model.predict({'Note': case, 'Amount': 500})
        print(f"  {case[:40]} â†’ {pred}")
    
    # â”€â”€ SAVE MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nðŸ’¾ Saving production model...")
    
    with open("../models/expense_model.pkl", 'wb') as f:
        pickle.dump(ensemble, f)
    
    with open("../models/tfidf_vectorizer.pkl", 'wb') as f:
        pickle.dump(tfidf, f)
    
    with open("../models/feature_scaler.pkl", 'wb') as f:
        pickle.dump(scaler, f)
    
    with open("../models/label_encoder.pkl", 'wb') as f:
        pickle.dump(label_encoder, f)
    
    # Save model info â€” cv fields removed since we removed cv
    model_info = {
        "accuracy": float(ensemble_score),
        "model_type": "Production Ensemble (Logistic + RF + GradientBoost + SVM)",
        "categories": label_encoder.classes_.tolist(),
        "training_samples": int(X_train.shape[0]),
        "test_samples": int(X_test.shape[0]),
        "features_count": int(X_combined.shape[1]),
        "improvement": f"+{(ensemble_score - 0.7417)*100:.1f}pp",
        "training_date": datetime.now().isoformat()
    }
    
    with open("../models/model_info.json", "w") as f:
        json.dump(model_info, f, indent=2)
    
    print("âœ… Production model saved successfully!")
    print(f"\nðŸŽ¯ PRODUCTION MODEL READY!")
    print(f"   Accuracy: {ensemble_score:.4f} ({ensemble_score*100:.2f}%)")
    
    return production_model

if __name__ == "__main__":
    train_production_model()